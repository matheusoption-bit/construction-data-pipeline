# DocumentaÃ§Ã£o: ImplementaÃ§Ã£o de UPSERT no SheetsLoader

## ğŸ“‹ Resumo

ImplementaÃ§Ã£o de lÃ³gica UPSERT (Update or Insert) no mÃ©todo `write_fact_series()` para evitar duplicaÃ§Ã£o de dados na aba `fact_series` do Google Sheets.

## ğŸ¯ Problema

**Antes:** O mÃ©todo `write_fact_series()` sempre adicionava dados usando `append_to_sheet()`, causando duplicaÃ§Ã£o quando o mesmo job executava mÃºltiplas vezes com os mesmos dados.

**Depois:** ImplementaÃ§Ã£o de UPSERT que identifica dados novos vs. atualizaÃ§Ãµes, remove duplicatas, e sobrescreve a aba apenas com dados dedupicados.

## ğŸ”„ MudanÃ§as Implementadas

### 1. Novo MÃ©todo: `read_fact_series()`

**LocalizaÃ§Ã£o:** `src/etl/sheets.py` (linha ~478)

**PropÃ³sito:** Ler dados existentes da aba `fact_series` e retornar como DataFrame pandas.

**Funcionalidades:**
- LÃª todos os dados da aba `fact_series`
- Converte para DataFrame com tipos corretos (numÃ©ricos para `valor`, `variacao_mom`, `variacao_yoy`)
- Retorna DataFrame vazio se aba nÃ£o existir ou estiver vazia
- Tratamento robusto de erros

**Exemplo de uso:**
```python
loader = SheetsLoader()
df_existing = loader.read_fact_series()
print(f"Registros existentes: {len(df_existing)}")
```

### 2. Novo MÃ©todo: `deduplicate_fact_series()`

**LocalizaÃ§Ã£o:** `src/etl/sheets.py` (linha ~541)

**PropÃ³sito:** Remover duplicatas do DataFrame por `id_fato`, mantendo o registro mais recente.

**Funcionalidades:**
- Remove duplicatas usando coluna `id_fato` como chave Ãºnica
- MantÃ©m registro mais recente baseado em `created_at`
- Retorna tupla `(DataFrame dedupicado, nÃºmero de duplicatas removidas)`
- Logging detalhado de duplicatas encontradas

**Exemplo de uso:**
```python
df_clean, removed = loader.deduplicate_fact_series(df)
print(f"Duplicatas removidas: {removed}")
```

### 3. MÃ©todo Modificado: `write_fact_series()`

**LocalizaÃ§Ã£o:** `src/etl/sheets.py` (linha ~597)

**MudanÃ§as principais:**

#### Antes (comportamento antigo):
```python
# âŒ Sempre adiciona dados (append), causa duplicaÃ§Ã£o
self.append_to_sheet("fact_series", rows)
```

#### Depois (comportamento novo):
```python
# âœ… UPSERT: identifica novos vs. atualizaÃ§Ãµes
df_existing = self.read_fact_series()

# Identificar IDs novos vs. atualizaÃ§Ãµes
truly_new_ids = new_ids - existing_ids
update_ids = new_ids & existing_ids

# Remover registros que serÃ£o atualizados
df_existing_filtered = df_existing[~df_existing['id_fato'].isin(update_ids)]

# Combinar existentes + novos
df_combined = pd.concat([df_existing_filtered, df_new], ignore_index=True)

# Deduplicar
df_final, duplicates_removed = self.deduplicate_fact_series(df_combined)

# Sobrescrever aba completamente com dados limpos
worksheet.clear()
worksheet.update('A1', all_data)
```

#### Fluxo Completo do UPSERT:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Preparar novos dados         â”‚
â”‚    - Adicionar id_fato          â”‚
â”‚    - Calcular variaÃ§Ãµes         â”‚
â”‚    - Adicionar metadados        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Ler dados existentes         â”‚
â”‚    - read_fact_series()         â”‚
â”‚    - Retorna DataFrame          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Identificar novos vs. update â”‚
â”‚    - Comparar id_fato           â”‚
â”‚    - truly_new_ids              â”‚
â”‚    - update_ids                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Combinar dados               â”‚
â”‚    - Remover IDs duplicados     â”‚
â”‚    - Concat existentes + novos  â”‚
â”‚    - Deduplicar                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Sobrescrever aba             â”‚
â”‚    - Clear worksheet            â”‚
â”‚    - Update com dados limpos    â”‚
â”‚    - Logging detalhado          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Logging Detalhado

O mÃ©todo agora loga informaÃ§Ãµes detalhadas sobre a operaÃ§Ã£o:

```python
logger.info(
    "fact_series_upsert_complete",
    series_id="ipca",
    exec_id="exec_20231101_120000",
    existing_rows=50,        # â† Quantas linhas jÃ¡ existiam
    new_rows=3,              # â† Quantas linhas sÃ£o realmente novas
    updated_rows=2,          # â† Quantas linhas foram atualizadas
    final_total=53,          # â† Total final apÃ³s UPSERT
    operation="upsert"
)
```

## ğŸ§ª Testes UnitÃ¡rios

**Arquivo:** `tests/test_sheets_upsert.py`

### Classes de Teste:

1. **TestReadFactSeries**
   - `test_read_fact_series_with_data`: Leitura de dados existentes
   - `test_read_fact_series_empty_sheet`: Aba vazia
   - `test_read_fact_series_not_found`: Aba nÃ£o encontrada

2. **TestDeduplicateFactSeries**
   - `test_deduplicate_with_duplicates`: Remove duplicatas corretamente
   - `test_deduplicate_no_duplicates`: NÃ£o altera dados sem duplicatas
   - `test_deduplicate_empty_dataframe`: Trata DataFrame vazio
   - `test_deduplicate_no_id_fato_column`: Trata ausÃªncia de coluna

3. **TestWriteFactSeriesUpsert**
   - `test_write_fact_series_no_existing_data`: InserÃ§Ã£o inicial
   - `test_write_fact_series_with_new_data`: Adiciona apenas novos
   - `test_write_fact_series_with_duplicates`: Atualiza duplicatas
   - `test_write_fact_series_missing_columns`: Valida colunas obrigatÃ³rias

4. **TestUpsertIntegration**
   - `test_multiple_series_upsert`: MÃºltiplas sÃ©ries simultaneamente

### Executar testes:

```bash
# Todos os testes de UPSERT
pytest tests/test_sheets_upsert.py -v

# Teste especÃ­fico
pytest tests/test_sheets_upsert.py::TestWriteFactSeriesUpsert::test_write_fact_series_with_duplicates -v

# Com coverage
pytest tests/test_sheets_upsert.py --cov=src.etl.sheets --cov-report=html
```

## âš ï¸ Breaking Changes

### Comportamento Modificado

**Antes:** `write_fact_series()` sempre adicionava dados ao final (append)

**Depois:** `write_fact_series()` sobrescreve aba completamente apÃ³s UPSERT

### Impacto

- âœ… **Positivo:** Elimina duplicaÃ§Ã£o de dados
- âœ… **Positivo:** Jobs podem rodar mÃºltiplas vezes sem criar duplicatas
- âš ï¸ **AtenÃ§Ã£o:** Aba Ã© sobrescrita completamente (nÃ£o afeta funcionamento normal)

### Compatibilidade

**CompatÃ­vel:** Nenhuma mudanÃ§a na assinatura do mÃ©todo:
```python
# Chamada permanece idÃªntica
loader.write_fact_series(series_id, data, exec_id)
```

**Quebra:** Nenhuma quebra de cÃ³digo existente.

## ğŸ”§ Performance

### OtimizaÃ§Ãµes Implementadas

1. **Batch Operations:** 
   - Usa `worksheet.update()` para escrever todos os dados de uma vez
   - Reduz chamadas Ã  API do Google Sheets

2. **Pandas Operations:**
   - DeduplicaÃ§Ã£o eficiente com `drop_duplicates()`
   - OperaÃ§Ãµes em memÃ³ria (rÃ¡pido para milhares de registros)

3. **Rate Limiting:**
   - MantÃ©m decorator `@rate_limit_api_call` existente
   - UPSERT adiciona apenas 1 chamada de leitura extra

### MÃ©tricas

| OperaÃ§Ã£o | Antes | Depois |
|----------|-------|--------|
| **API Calls** | 1 (append) | 2 (read + update) |
| **Duplicatas** | âŒ Sim | âœ… NÃ£o |
| **Tempo** | ~0.5s | ~1.0s (com leitura) |

## ğŸ“ Exemplos de Uso

### Exemplo 1: Primeira InserÃ§Ã£o

```python
from src.etl.sheets import SheetsLoader
import pandas as pd

loader = SheetsLoader()

# Dados novos
df = pd.DataFrame({
    'data_referencia': ['2023-01-01', '2023-02-01'],
    'valor': [100.5, 102.3]
})

# ExecuÃ§Ã£o
loader.write_fact_series('ipca', df, 'exec_001')

# Log:
# existing_rows=0, new_rows=2, updated_rows=0, final_total=2
```

### Exemplo 2: AtualizaÃ§Ã£o de Dados

```python
# Executar novamente com mesmos dados + novos
df = pd.DataFrame({
    'data_referencia': ['2023-01-01', '2023-02-01', '2023-03-01'],
    'valor': [100.5, 102.8, 103.5]  # valor de 02-01 atualizado
})

loader.write_fact_series('ipca', df, 'exec_002')

# Log:
# existing_rows=2, new_rows=1, updated_rows=2, final_total=3
```

### Exemplo 3: MÃºltiplas SÃ©ries

```python
# SÃ©rie 1: IPCA
df_ipca = pd.DataFrame({
    'data_referencia': ['2023-01-01'],
    'valor': [100.5]
})
loader.write_fact_series('ipca', df_ipca, 'exec_003')

# SÃ©rie 2: SELIC (nÃ£o afeta IPCA)
df_selic = pd.DataFrame({
    'data_referencia': ['2023-01-01'],
    'valor': [13.75]
})
loader.write_fact_series('selic', df_selic, 'exec_003')

# Resultado: fact_series contÃ©m ambas sÃ©ries sem duplicatas
```

## ğŸ” VerificaÃ§Ã£o

### Validar UPSERT funcionando

1. **Executar job duas vezes:**
```bash
python -m src.jobs.daily_bcb
python -m src.jobs.daily_bcb  # Segunda execuÃ§Ã£o
```

2. **Verificar no Google Sheets:**
   - Abrir aba `fact_series`
   - Contar linhas para cada `series_id`
   - âœ… NÃ£o deve haver duplicatas de `id_fato`

3. **Verificar logs:**
```bash
grep "fact_series_upsert_complete" logs/*.log
```

### Query para verificar duplicatas:

Se tiver acesso ao Sheets como DataFrame:
```python
df = loader.read_fact_series()
duplicates = df[df.duplicated(subset=['id_fato'], keep=False)]
print(f"Duplicatas encontradas: {len(duplicates)}")
# Deve retornar 0
```

## ğŸš€ PrÃ³ximos Passos

1. âœ… CÃ³digo implementado
2. âœ… Testes unitÃ¡rios criados
3. âœ… DocumentaÃ§Ã£o completa
4. â³ **Executar testes:** `pytest tests/test_sheets_upsert.py -v`
5. â³ **Testar em produÃ§Ã£o:** Executar `daily_bcb` job duas vezes
6. â³ **Monitorar logs:** Verificar mÃ©tricas de UPSERT
7. â³ **Validar dados:** Confirmar ausÃªncia de duplicatas no Sheets

## ğŸ“š ReferÃªncias

- **Arquivo modificado:** `src/etl/sheets.py`
- **Testes:** `tests/test_sheets_upsert.py`
- **DocumentaÃ§Ã£o mÃ³dulo:** Docstrings nos mÃ©todos
- **Logs:** Procurar por `fact_series_upsert_complete` nos logs

## â“ FAQ

**P: O UPSERT Ã© mais lento que append?**
R: Sim, ~2x mais lento devido Ã  leitura extra, mas elimina duplicatas. Performance aceitÃ¡vel para datasets tÃ­picos (< 10.000 registros).

**P: E se dois jobs executarem simultaneamente?**
R: Ãšltima escrita vence (last-write-wins). Para ambientes crÃ­ticos, considere lock distribuÃ­do.

**P: Posso desabilitar UPSERT?**
R: NÃ£o hÃ¡ flag para desabilitar. Para append puro, use `append_to_sheet()` diretamente (nÃ£o recomendado).

**P: Como saber se hÃ¡ duplicatas antigas no Sheets?**
R: Execute `loader.read_fact_series()` e use `df.duplicated(subset=['id_fato'])` para verificar.

## ğŸ“„ LicenÃ§a

Este cÃ³digo Ã© parte do projeto `construction-data-pipeline`.
